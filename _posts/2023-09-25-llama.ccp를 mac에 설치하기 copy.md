---
layout: post
title: llama.ccp를 mac에 설치하기
description: llama.ccp
summary: 
tags: NLP
minute: 1
---

## **[[라마]]([[llama]])가 뭔데?**

​

이 [[모델]]은 [[META]]에서 선보인 [[언어 모델]]로, 여러 특징들이 있지만 그 중 몇 가지만 간략하게 소개하겠습니다.

​

1.  **[[트랜스포머]] 기반**: [[LLaMA]]는 현대의 대부분 언어 모델과 같이 트랜스포머 아키텍쳐를 기반으로 합니다.

2.  **퍼블릭 데이터 사용**: 이 모델의 독특한 점 중 하나는 퍼블릭 데이터만을 사용하여 [[훈련]]된다는 것입니다. 즉, 비공개 데이터나 문서화되지 않은 데이터에 의존하지 않습니다.

3.  **다양한 크기의 모델**: LLaMA는 7B부터 65B까지 다양한 크기의 [[파라미터]]로 훈련된 모델 버전을 제공합니다.

4.  **뛰어난 성능**: 특히 LLaMA-13B 모델은 [[GPT-3]]의 성능을 능가하는 반면, 크기는 GPT-3의 1/10에 불과합니다. 이는 연구와 개발에 있어 더욱 효율적인 활용을 가능하게 합니다.

5.  **아카데믹한 접근**: LLaMA는 아카데믹한 관점에 더 많은 초점을 맞추고 있어, [[연구]]자들에게 큰 관심을 받고 있습니다.

​

해당 모델은 [[OpenAI]]에서 나온 [[ChatGPT]] 등 많은 [[LLM]] task에서 model size를 키워나가는 연구동향에서 parameter의 수를 줄이고 더 좋은 성능을 낼 수 없을까 라는 생각에서 연구를 시작했다고 합니다.

​

현재도 [[open source]]로서 모델을 배포하고 있어 많은 연구자들에게 도움이 되고 있는 와중입니다.
​

---


### llama를 local에서 돌리고 싶은데... GPU가 없네?

​

현재 제 노트북은 apple silicon M1 macbook으로 llm을 돌릴 수 있는 환경(즉, GPU를 활용한 cuda 세팅이 안됨)이 되지 않는 상태입니다.

​

따라서 [[NLP]] 연구를 [[local]]에서 진행하기에 쉽지 않은 부분이 많았습니다.

​

하지만! [[llama.cpp]]는 4비트 정수 [[양자화]]를 이용해서 맥북에서 Llama 모델을 실행하는 것을 목표로 만들어진 프로젝트입니다.

​

[[HuggingFace]]의 [TheBloke/Llama-2-7B-GGML](https://huggingface.co/TheBloke/Llama-2-7B-GGML)에서 양자화([[GGML]])된 Llama 2 모델을 다운로드 받을 수 있습니다.

​

---

​

## 🔍 **LLaMA 모델을 [[Mac]]에서 실행하기: 단계별 가이드**

​

Meta에서 발표한 LLaMA 모델을 Mac에서 실행하는 방법에 대해 자세히 알아보겠습니다. 아래는 단계별 설치 및 실행 방법입니다.

​

### 설치 전 단계

​

먼저, 필요한 도구들을 설치하기 전에 아래의 프로그램들이 설치되어 있는지 확인해주세요.

​

#### Command Line Tools 설치

​

터미널을 열고 아래의 명령어를 입력하세요:

​

```

xcode-select --install

```

​

대부분의 경우 문제 없이 설치될 것입니다. 하지만 설치에 문제가 발생한다면, 애플 개발자 계정을 통해 직접 다운로드 받아야 합니다.

​

1.  [Apple Developer](https://developer.apple.com/) 페이지에 접속하여 로그인 후 Apple Developer Account를 설정합니다.

2.  사유는 간단하게 작성하면 승인 이메일이 곧 도착할 것입니다. (APPLE DEVELOPER PROGRAM은 선택하지 마세요.)

3.  승인 후, [More - Downloads - Apple Developer](https://developer.apple.com/download/more/) 페이지에서 Command Line Tools를 직접 다운로드하여 설치합니다.

​

#### Homebrew 설치

​

```

/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

```

​

#### Git 설치

​

```

brew install git

```

​

### 파이썬 설치

​

LLaMA 모델을 사용하기 위해선 파이썬이 필요합니다. 아래의 링크에서 파이썬을 다운로드 받아 설치하세요.

​

[Python Downloads](https://www.python.org/downloads/)

​

설치 후, 아래의 명령어를 터미널에 입력합니다:

​

```

python3 -m pip install torch numpy sentencepiece

```

​

만약 pip가 설치되어 있지 않다면, 아래의 명령어로 설치할 수 있습니다:

​

```

sudo easy_install pip

```

​

###  llama.cpp 다운 및 컴파일

​

터미널에서 아래의 명령어를 순서대로 입력하여 llama.cpp를 다운로드하고 컴파일합니다:

​

```

git clone https://github.com/ggerganov/llama.cpp

cd llama.cpp

make

```

​

### 실행

​

#### 텍스트 파일 생성하기

​

1.  **텍스트 편집기 사용하기**: Mac의 기본 텍스트 편집기를 열고, 새로운 문서를 생성합니다.

2.  **포멧 설정하기**: 편집기 상단바에서 '포멧'을 클릭한 후 '일반 텍스트 만들기'를 선택합니다. 이렇게 하면 텍스트 파일이 일반 텍스트 형식으로 저장됩니다.

3.  **텍스트 복사 및 저장**: 아래의 텍스트를 복사하여 편집기에 붙여넣습니다.

​

```

A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.

​

### Human: Hello, Assistant.

​

### Assistant: Hello. How may I help you today?

​

### Human: Please tell me the largest city in Europe.

​

### Assistant: Sure. The largest city in Europe is Moscow, the capital of Russia.

​

### Human:

```

​

4.  **파일 저장**: 복사한 텍스트를 `vicuna.txt`라는 이름으로 저장합니다. 그리고 이 파일을 `llama.cpp`의 `prompts` 폴더에 넣습니다.

​

---

​

### 모델 실행하기

​

1.  **터미널에서 디렉토리 이동**: 터미널을 열고 `cd llama.cpp` 명령어를 입력하여 `llama.cpp` 디렉토리로 이동합니다.

2.  **모델 실행 명령어 입력**: 아래의 명령어를 터미널에 입력하여 모델을 실행합니다.

​

```

./main -m ./models/Wizard-Vicuna-7B-Uncensored.ggmlv3.q5_1.bin --mirostat 2 --color --mlock -t 2 --repeat_last_n -1 --repeat_penalty 1.2 --ignore-eos --temp 0.8 -b 512 -r "### Human:" -i -f prompts/vicuna.txt

```

​

3.  **모델 재실행**: 나중에 모델을 다시 실행하려면, `cd llama.cpp`를 입력한 후 위의 실행 명령어를 다시 입력하면 됩니다.

4.  **주의사항**: 가끔 모델이 예상대로 응답하지 않을 때가 있습니다. 이럴 경우, 실행 명령어 끝에 `-ins`를 추가하고 실행합니다. 로딩이 완료되어 `### Human:`이 표시되면 바로 `hello`라고 입력하고 엔터를 누릅니다. 이렇게 하면 모델이 정상적으로 작동합니다.

​

---

​

이상으로 LLaMA 모델을 Mac에서 실행하는 방법에 대해 알아보았습니다. 추가적인 모델이나 정보는 [GitHub](https://github.com/underlines/awesome-marketing-datascience)에서 확인하실 수 있습니다. RAM이 부족할 경우 `--mlock` 옵션을 조절하여 사용하세요.