---
title:  "Feature Extraction 2nd"
excerpt: "Feature Extraction"

categories:
  - Machine Learning
tags:
  - [Machine Learning, Data Science]

use_math: true
toc: true
toc_sticky: true
 
date: 2023-02-22
last_modified_at: 2023-02-22
---
# Features
## Handling Nonlinearity with Linear Methods
### Example Task: Predicting Health
예시를 한 번 들어보겠습니다.<br>
일반적으로 우리는 의료 진단용으로 가능성이 있는 모든 feature들을 추출합니다.<br>
- ex) height, weight, body temperature, blood pressure, etc...

### Issues for Linear Predictors
선형 예측 모델에서, 특징(feature)이 어떻게 추가되는지가 중요합니다.<br>
문제를 일으킬 수 있는 비선형성(nonlinearity)의 세 가지 유형은 다음과 같습니다:<br>

1. **단조성이 아님(non-monotonicity)**
2. **포화(saturation)**
3. **특징 간 상호작용(interactions between features)**

이러한 비선형성(nonlinearities)이 있을 경우, 모델이 예측을 위해 사용하는 특징(feature)들의 중요도나 가중치를 잘못 판단할 수 있습니다.<br>
이는 모델의 정확성과 일반화 능력을 떨어뜨릴 수 있습니다.<br>

>"단조성이 아님(non-monotonicity)", "포화(saturation)", "특징 간 상호작용(interactions between features)"은 모두 비선형(nonlinear)적인 관계를 나타내는 특징입니다.<br>
"단조성이 아님(non-monotonicity)": 두 변수 사이의 관계가 단조적(monotonic)이지 않은 경우를 의미합니다. 즉, 한 변수가 증가할 때 다른 변수의 값이 단조적으로 증가하거나 감소하지 않는 경우입니다.<br>
"포화(saturation)": 입력값이 일정 수준 이상이 되면 출력값이 더 이상 증가하지 않고 일정 수준을 유지하는 경우를 말합니다. 이는 종종 물리적인 한계나 한계점을 나타내는 경우가 많습니다.<br>
"특징 간 상호작용(interactions between features)": 하나의 특징이 다른 특징에 영향을 미치는 경우를 의미합니다. 이러한 상호작용은 종종 두 특징을 더해주거나 곱해주는 등의 방식으로 모델에 반영됩니다.

### Non-monotonicity: The Issue
- **단조성이 아닌(non-monotonic)** 특징은 특정 값의 범위에서는 결과에 부정적인 영향을 끼칠 수 있지만 다른 값의 범위에서는 결과에 긍정적인 영향을 끼칠 수 있습니다.<br>
단조적(monotonic)이라는 용어는 기울기가 항상 증가하거나 항상 감소하는 선형 관계를 의미하는 것은 아닙니다.<br>
단조적이라는 말은 단지 변수 간의 관계가 항상 같은 방향으로 움직인다는 것을 의미합니다.<br> 예를 들어, x가 증가할 때 y도 항상 증가하면 x와 y 사이의 관계는 단조적입니다.<br> 하지만 x가 증가할 때 y가 먼저 감소하다가 다시 증가한다면 이는 비단조적인 관계입니다.
따라서, Non-monotonicity 문제는 변수 간의 관계가 비선형이거나 불규칙한 경우에도 발생할 수 있습니다.<br>
예를 들어, 당신이 주식 투자를 한다고 가정해 봅시다.<br>
당신은 어떤 회사의 주식 가격이 상승할 것이라는 예측을 하고 그 주식을 매수합니다.<br> 그러나 나중에 그 회사에서 부정한 일이 드러나면, 그 주식 가격은 급격히 하락할 것입니다.<br> 이 때, 당신의 예측이 잘못되어서 이 주식을 매수한 것이므로, 이 문제는 Non-monotonicity 문제입니다.<br> 
이는 변수인 "예측"과 "실제 주식 가격" 간의 비선형적이고 불규칙한 관계 때문에 발생한 것입니다.<br>
이 경우 모델은 이러한 특징의 중요도를 적절하게 평가하기 어려울 수 있습니다.<br>
Non-monotonicity가 발생하는 경우에는 변수 간에 선형 관계가 아닌, 비선형 관계가 존재하거나, 우리가 알지 못하는 다른 변수가 영향을 미치는 경우가 있을 수 있습니다.<br> 이런 경우에는 적절한 feature engineering이나 다른 기술적인 방법을 사용하여 모델을 개선해야 합니다.

- Feature Map: $φ(x) = [1,temperature(x)]$
- Action: Predict health score $y ∈ R$ (positive is good)
- Hypothesis Space $F = {affine\ functions\ of\ temperature}$

이 예시에서는 온도(temperature) 값을 이용하여 건강 점수$(y)$를 예측하는 모델을 고려합니다.<br>
모델은 온도 값의 선형 함수로 구성되며, 이를 수식으로 나타내면 $φ(x) = [1,temperature(x)]$로 표현됩니다.<br>

여기서 $φ(x)$는 입력 $x$의 특징 벡터(feature vector)를 나타내며, $[1,temperature(x)]$는 상수 1과 입력 x의 온도 값으로 이루어져 있습니다.<br>
그러나 건강 점수는 온도의 선형 함수가 아닙니다.<br>
따라서 온도와 건강 점수 사이에는 선형 관계가 존재하지 않습니다.<br>
이 모델에서 선형 함수를 사용하면, 높은 온도와 낮은 온도 모두 건강에 좋지 않은 영향을 미친다는 문제가 발생합니다.<br>
선형 함수는 높은 온도가 나쁘다고 하면, 낮은 온도는 좋다고 말할 수밖에 없기 때문입니다.<br>

하지만 이 경우에는 낮은 온도와 높은 온도 모두 건강에 나쁜 영향을 미치는 것입니다.<br>
따라서 이러한 문제를 해결하려면 선형 함수 대신에 비선형 함수를 사용해야 합니다.<br>
이 예시에서는 온도와 건강 점수 사이에 비선형 관계가 존재하므로, 선형 함수로는 이를 모델링할 수 없습니다.<br>

### Non-monotonicity: Solution 1
- Transform the input: $φ(x) = [1,{temperature(x)-37}^2]$ <br>

위의 예시에서는 체온(temperature)이라는 feature로부터 건강 점수(health score)를 예측하는 문제를 다루고 있습니다.<br>
그러나 건강 점수는 체온의 선형 함수로는 정확하게 예측할 수 없는 경우였습니다.<br> 따라서 이 문제를 해결하기 위해 체온의 변형을 feature로 사용하는 것이 제안되었습니다.<br>

이를 위해 입력 $x$를 변형하여 $φ(x) = [1, (temperature(x) - 37)^2]$ 로 만들었습니다.<br> 이제 건강 점수 $y$는 $φ(x)$를 사용하여 예측됩니다.<br>
그러나 이러한 입력 변형은 전문 지식이 필요한 경우가 있습니다.<br>

예를 들어, 위의 예시에서는 정상 체온이 37℃인 것을 알아야 합니다.<br>
따라서 이러한 변형은 도메인 전문가의 도움이 필요합니다.<br>
이러한 변형을 사용하지 않고도 모델을 학습할 수 있으나, 때로는 입력에 대한 도메인 지식이 모델의 예측 능력을 향상시키는 데 도움이 될 수 있습니다.<br>

### Non-monotonicity: Solution 2

- $φ(x) = [1,temperature(x) ,(temperature(x))^2]$

기존에 가지고 있는 feature들을 최대한 활용해서 solution1보다 표현력이 뛰어나게끔 만들어준 mapping 함수입니다.<br>
이 경우, 예시로 들어진 feature인 온도(temperature)를 그대로 사용하지 않고 온도의 제곱도 함께 feature로 추가하는 것입니다.<br>
이렇게 하면 모델이 **더 복잡한 패턴도 학습할 수 있게 됩니다.** <br>

일반적으로 좋은 feature는 간단하면서도 중요한 정보를 포함하는 것입니다.<br>
이러한 feature들을 조합하여 모델을 만들면 더 복잡한 패턴을 학습할 수 있습니다.<br>
따라서 feature는 단순하고 각 *feature가 독립적으로 중요한 정보*를 제공하는 것이 좋습니다.<br>

> 비선형성이 있는 경우에도 **feature extraction이 적용** 이 되는지?<br>
  비선형성이 있는 경우에도 feature extraction은 적용됩니다. 사실, 비선형성이 있는 경우에 feature extraction이 더욱 중요해집니다.<br>
  비선형성이 있으면, 입력과 출력 간의 관계가 복잡하게 되어서, 이를 표현하기 위해 더 많은 feature가 필요합니다.<br>
  이를 위해, 보다 복잡한 feature extraction 기법이 필요할 수 있습니다.<br>
  예를 들어, 비선형 관계가 있는 경우, 다항식 특성(polynomial features)을 추가할 수 있습니다.<br>
  이를 통해 비선형성을 나타내는 새로운 feature를 만들어 내어 모델의 표현력을 높일 수 있습니다.<br>
  또한, 신경망 같은 비선형 모델에서는 더 복잡한 feature extraction 기법이 사용될 수 있습니다.<br>

### Saturation: The Issue
- **포화(saturation)** 가 발생하는 특징은 값이 일정 범위를 넘어설 경우, 결과에 더 이상 긍정적인 영향을 끼치지 않는 경향이 있습니다.<br>
Saturation은 입력값이 일정 수준 이상이 되면 출력값이 더 이상 증가하지 않는 현상을 말합니다.<br>
 예를 들어, 어떤 자전거 브레이크의 제동력이 브레이크 패드와 바닥 사이의 마찰 계수에 비례한다고 가정해보겠습니다.<br>
 그렇다면, 마찰 계수가 일정 수준 이상이 되면 브레이크 제동력이 더 이상 증가하지 않을 것입니다.<br>
 이러한 경우에는 선형 모델이나 로지스틱 회귀 모델 같은 단순한 모델을 사용하면 제대로된 결과를 얻을 수 없습니다.<br>
 이런 경우에는 적절한 모델링 기법이나 feature engineering 기술을 사용하여 모델을 개선해야 합니다.<br>
이 경우 모델은 특정 값 이상의 특징을 무시하거나, 잘못된 가중치를 부여할 수 있습니다.<br>

- Setting: 사용자의 검색어와 관련된 제품을 찾습니다.<br>
- Input: Product $X$<br>
- Action: $X$의 검색어와의 관련성을 점수로 평가합니다.<br>
- Feature Map: $φ(x) = [1,N(x)]$, where $N(x)=$ $x$를 구매한 사람들의 숫자 입니다.

Feature Map에서와 같이 $N(x)=$인 feature를 추가해서 비선형적인 패턴을 추출하겠다는 이야기 입니다.<br> 
이 경우 우리는 $N(x)$와 $x$의 검색어와의 관련성 사이에서 단조적인 관계가 있을 것으로 예상하지만, 실제로는 아닐 수 있습니다.<br>
그 이유는 관련성 점수는 **관련성 예측에 대한 신뢰도** 를 반영하기 때문입니다.<br>

구매횟수가 매우 큰 경우에 feature에 대한 정보를 잘못해석해서 예측이 어려워지는데 이것을 우리는 **Saturation** 이라고 합니다.<br>
따라서 우리는 feature에 대한 정보를 더 잘 반영할 수 있는 방식으로 feature map을 설계하면 saturation을 피할 수 있습니다.<br>

즉, $N(x)$가 1000이 되었다고 해서($x$를 구매한 사람이 1000명 이라고 해서)$N(x)$가 100일 때 보다 10배 더 신뢰할 수 있다는 것이 아니라는 말입니다.<br>
그렇기 때문에 $N(x)$가 커질수록 관련성 점수도 증가하지만, **선형적으로 증가하는 것이 아닙니다.** (비선형적인 것을 handling해야하는 이유)<br>

데이터가 많은 것은 좋지만 그렇다고 해서 구매한 사람 수가 많을 수록 관련성 점수와의 관계가 선형적이지 않을 수 있습니다.<br>
구매한 사람의 수가 늘어날수록 관련성 예측에 대한 신뢰도는 높아지지만 그 차이가 크지 않다는 것을 의미합니다.<br>

### Saturation: Solve with nonlinear transform
- $φ(x) = [1,log {1+N(x)}]$

Saturation이 발생하는 경우에는, 입력값과 출력값의 관계가 선형적이지 않아서 원하는 결과를 얻을 수 없게 됩니다.<br> 
이럴 때에는 입력값에 대해 비선형 변환을 적용하여 문제를 해결할 수 있습니다.<br>

예를 들어, 제품 검색에서 특정 제품과 검색어의 관련성을 예측할 때, 제품이 구매된 횟수를 입력값으로 사용할 수 있습니다.<br>
하지만 구매 횟수가 큰 차이를 보이는 경우(1000번과 100번의 예시), 입력값과 관련성 점수 간에 선형적인 관계를 가정하는 것이 어렵습니다.<br>
따라서 이 때에는 로그 함수와 같은 비선형 함수를 사용하여 입력값을 변환하면 선형적인 관계를 더 잘 모델링할 수 있습니다.<br>
예를 들어, $[1, log(1+N(x))]$와 같은 비선형 특징 맵을 사용할 수 있습니다.<br>
이때, 로그 함수의 밑(base)은 어떤 값이던지 상관없이 원하는 결과를 얻을 수 있습니다.<br>

### Saturation: Solve by discretization

Discretization은 연속적인 값을 구간별로 나누어 각 구간에 해당하는 이산적인 값으로 변환하는 과정을 말합니다.<br>
이산적인 값을 다루는 것이 훨씬 더 쉽고 간편하기 때문에(연속적인 값을 다루려면 무한한 경우의 수를 고려해야함), 이산적인 값을 다루는 것이 더 효율적일 때 사용됩니다.<br>

- Discretization (a discontinuous transformation):<br>
  $φ(x) = (1(5 \le N(x) < 10),1(10 \le N(x) < 100),1(100 \le N(x)))$

- Sometimes we might prefer one-sided buckets:<br>
  $φ(x) = (1(5 \le N(x)),1(10 \le N(x)),1(100 \le N(x)))$

이 방법은 구매 횟수(N(x))에 따라 몇 개의 분리된 버킷(buckets)으로 나누는 것입니다.<br> 여기서, 각 버킷은 구매 횟수가 특정 구간에 속하는 제품들을 나타냅니다.<br>

예를 들어, 첫 번째 예시에서는 5회 이상 10회 미만으로 구매된 제품은 1번째 버킷, 10회 이상 100회 미만으로 구매된 제품은 2번째 버킷, 100회 이상으로 구매된 제품은 3번째 버킷에 속합니다.<br>
이 방법은 몇몇 버킷들을 제외하고는 비선형적인 관계를 표현할 수 있습니다.<br>

하지만, 모든 버킷의 크기가 비슷하도록 하기 위해 한쪽 방향으로만 버킷을 나누는 경우가 있습니다.<br>
예를 들어, 두 번째 예시에서는 각 버킷의 크기가 달라지도록 5회 이하로 구매된 제품은 1번째 버킷, 10회 이하로 구매된 제품은 2번째 버킷, 100회 이하로 구매된 제품은 3번째 버킷에 속하게 됩니다.<br>

이 방법은 구매 횟수가 적은 제품에 대한 모델의 가중치를 규제할 수 있어 일반화 성능을 높일 수 있습니다.<br>

> **가중치** 는 어떻게 **규제** 하는데?<br>
  위의 모델에서 사용하는 구매횟수(n(x)) 변수는 값이 연속적이지 않고, 몇 개의 구간으로 나누어져 이산화(discretization)되어 있습니다.<br>
  예를 들어, 5 이상 10 미만인 구매 횟수를 가진 제품은 첫 번째 버킷(구간)에 속하며, 10 이상 100 미만인 구매 횟수를 가진 제품은 두 번째 버킷에 속합니다.<br>
  그러나 이러한 구간은 실제 데이터의 분포에 따라 큰 차이를 보일 수 있습니다.<br>
  예를 들어, 구매횟수가 100 이상인 제품은 적은 비중을 차지할 수 있습니다.<br>
  따라서 이러한 경우, 두 번째 모델과 같이 한쪽 방향으로만 버킷을 만드는 것이 더 적합할 수 있습니다.<br>
  또한, Saturation은 모델의 가중치를 규제하는 방법 중 하나입니다.<br>
  이 방법은 **구매횟수가 적은 제품** 에 대해 모델의 가중치를 규제하여 *일반화 성능* 을 높이는 것입니다.<br>
  예를 들어, 구매횟수가 5인 제품과 같은 희귀한 경우에는 구매횟수가 더 높은 제품에 비해 더 큰 가중치를 주는 것을 방지할 수 있습니다.<br>
  즉, *구매횟수 분포에 따라서 적합한 버킷 방식* 을 선택하고, Saturation을 적용하는 것은 모델의 일반화 성능을 높이는 방법 중 하나입니다.<br>
  이를 통해 구매횟수 분포가 크게 차이 나는 데이터에 대해 더 정확한 예측을 할 수 있습니다.<br>

하지만, 버킷의 수가 많아지면 *모델의 복잡도* 가 증가하므로 과적합 문제를 방지하기 위해 조절이 필요합니다.<br>

위에서 주어진 feature map인 $φ(x) = [1, N(x)]$에서, 구매 횟수인 $N(x)$를 그대로 사용하면 모델이 구매 횟수에 대한 선형적인 관계만 학습할 수 있습니다.<br>

하지만, 구매 횟수를 여러 개의 구간으로 나누어 이산적인 값으로 만들면, 각 구간에 따라 다른 가중치를 학습하여 **더 복잡하고 유연한 모델** 을 만들 수 있습니다.<br>

위의 예시에서는 3개의 구간으로 나누어 $φ(x)$를 정의했습니다.<br>
이러한 변환 방법을 **"discretization"** 이라고 부르며, 이산적인 값으로 변환하여 모델을 더 유연하게 만들 수 있습니다.<br>

> 모델을 유연하게 만들면 *장점* 이 뭔데?<br>
  모델을 유연하게 만들어주는 것은 여러 가지 장점을 가질 수 있습니다.<br>
  첫째, 데이터의 복잡도에 더욱 잘 적응할 수 있습니다.<br>
  모델이 더 유연해지면 데이터의 미세한 패턴을 더 잘 파악하고 예측할 수 있습니다.<br> 둘째, 모델이 일반화 성능을 더 잘 발휘할 수 있습니다.<br>
  데이터에 과적합되는 것을 방지하여 새로운 데이터에 대한 예측 성능을 높일 수 있습니다.<br> 
  셋째, 다양한 feature를 적용하여 모델의 표현력을 높일 수 있습니다.<br>
  더 다양한 feature를 사용하면 모델이 다양한 패턴을 학습할 수 있어 예측 성능이 높아질 수 있습니다.<br>

### Interactions: The Issue
-  **특징 간 상호작용(interactions between features)** 이 있을 경우, 두 개 이상의 특징이 함께 사용될 때 결과가 예상과 다를 수 있습니다.<br> 
Interactions between features는 두 개 이상의 feature가 함께 사용될 때 예측값에 영향을 미치는 경우를 의미합니다.<br>
예를 들어, 성별과 키 두 가지 feature가 있다고 가정해보겠습니다.<br>
만약 성별에 따라 키에 대한 평균값이 다르다면, 성별과 키 두 가지 feature가 함께 사용될 때 예측값에 영향을 미치게 됩니다.<br>
이러한 경우, 두 feature를 모두 고려해야 더 정확한 예측값을 얻을 수 있습니다.<br>
하지만 때로는 두 feature가 함께 사용될 때 예측값에 영향을 미치지 않는 경우도 있습니다.<br>
예를 들어, 온도와 강수량이라는 두 feature가 있다고 가정해보겠습니다.<br> 온도가 높아지면서 강수량이 많아진다고 해도, 강수량이 높은 경우 온도가 높아질 수도 있고 낮아질 수도 있기 때문에 두 feature가 함께 사용될 때 예측값에 큰 영향을 미치지 않을 수 있습니다.<br>
Interactions between features는 feature engineering 과정에서 매우 중요합니다.<br> 모델이 두 feature 간의 interaction을 고려하지 않는다면, 모델이 예측할 수 있는 범위가 한정될 수 있습니다.<br>
따라서 feature engineering 과정에서는 모든 가능한 interaction을 고려하여 예측값에 미치는 영향을 파악하고, 이를 모델에 반영하여 보다 정확한 예측을 할 수 있도록 해야 합니다.<br>
교호작용은 하나의 feature가 반영하는 정보가 다른 feature들에 따라서 영향을 받는 경우를 의미합니다.<br>
예를 들어, 날씨와 교통량이라는 두 feature가 있을 때, 날씨가 좋을 때는 교통량이 많아지는 경향이 있고, 날씨가 나쁠 때는 교통량이 감소하는 경향이 있다면, 날씨와 교통량은 교호작용을 가지고 있다고 할 수 있습니다.<br>
교호작용은 모델의 성능을 높이기 위해서 중요한 정보이며, feature engineering 단계에서 교호작용을 고려하여 새로운 feature를 만들어내는 것이 일반적입니다.<br>
이 경우 모델은 이러한 상호작용을 적절하게 고려하지 않을 경우 예측 결과가 부정확해질 수 있습니다.<br>

- Input: Patient information $x$
- Action: Health score $y ∈ R$ (높을수록 좋습니다.)
- Feature Map: $φ(x) = [height(x),weight(x)]$
- Issue: height에 관련된 weight가 중요합니다.

이러한 관계는 linear한 분류로는 해결할 수 없습니다.<br>
즉, 높은 height에 대한 높은 wright는 높은 건강 점수를 가져오지만, 낮은 hright에 대한 낮은 weight는 건강 점수를 낮추는 것이므로, 이러한 상호작용을 고려해야합니다.<br>

이러한 상호작용을 고려하기 위해서는 둘을 곱한 값을 새로운 특성으로 추가하거나, 다항 특성(polynomial feature)을 추가하는 등의 방법으로 feature map을 구성해야합니다.<br>

위와 같은 방법을 통해서 더 복잡한 모델을 만들어서 height와 weight 사이의 상호작용을 고려할 수 있으며 더 높은 성능을 당성할 수 있습니다.<br>

### Interactions: Approach 1
상호작용을 고려하는 예시로서 구글에서 검색을 한"키에 따른 이상체중"을 예시로 설명할 수 있습니다.<br>
J.D.로빈손의 "이상체중"공식은 다음과 같습니다(남성기준):<br> 

$ideal\ weight(kg) = 52+1.9[height(inch)−60]$ <br>

이 수식을 이용하여 각 환자의 이상 체중을 계산하고, 실제 체중과의 차이를 제곱한 값을 $f(x)$로 정의합니다.<br>
이 때, $f(x)$는 다음과 같은 복잡한 수식이 됩니다.<br>

$f(x) = (52+1.9[h(x) −60]−w(x))^2$ <br>

위의 수식은 환자의 실제 체중$(w(x))$와 J.D.로빈슨의 이상 체중 공식을 이용하여 계산한 이상 체중$(52+1.9[h(x) −60])$과의 차이를 제곱한 값을 $f(x)$로 정의하는 수식입니다.<br>
이렇게 정의된 $f(x)$값은 환자의 실제 체중과 이상 체중의 차이를 반영하는 점수로 사용될 수 있습니다.<br>

제곱을 취함으로써 차이가 큰 값들이 더 큰 영향을 미치게 되어, 이상 체중에 더 가까운 환자는 높은 점수를, 실제 체중이 이상 체중보다 높은 환자는 낮은 점수를 받게 됩니다.<br> 이렇게 점수를 산출함으로써, 상호작용을 고려한 더 정확한 건강 점수 모델을 만들 수 있습니다.<br>

WolframAlpha라는 검색엔진을 통해 복잡한 수학식을 풀어본다면 다음과 같습니다.<br>

$f (x) = 3.61h(x)^2 −3.8h(x)w(x) −235.6h(x) +w(x)^2 +124w(x) +3844$

### Interactions: Approach 2
상호작용에서의 두번째 접근방법은 모든 2차항의 특징들을 포함하는 것 입니다.<br>

$φ(x) = [1,h(x),w(x),h(x)^2,w(x)^2, h(x)w(x)(교차 항)]$<br>

이 때의 mapping function $φ(x)$ 은 앞서 나온 비선형적 요소들과 같이 polynomial 형식입니다.<br>
앞서 설명한 접근법 1과 같은 방법은 구글 검색이나 WolframAlpha를 사용하여 이상적인 체중을 계산하고, 특정 상호 작용을 계산하기 위해 해당 체중과 키와의 제곱 차이를 계산했습니다.<br>
이 방법은 매우 유연합니다.<br>
각 원시 특징에 대한 제곱 및 교차 항을 포함하여 모든 2 차원 특징을 포함할 수 있으며, 따라서 **선형 모델에서도 복잡한 비선형 상호 작용을 모델링** 할 수 있습니다.<br>
이 방법을 사용하면 모델이 자체적으로 모든 상호 작용을 학습하므로 사용자는 특정 상호 작용을 찾을 필요가 없습니다.<br>
또한, 이 방법은 **외부 도구를 사용하지 않으므로** 이전 방법보다 더 간단하며 유연합니다.<br> 이것은 또한 개발자가 복잡한 함수 및 외부 도구를 사용하여 일부 상호 작용을 추가 할 때 발생할 수 있는 오류를 방지합니다.<br>

### Predicate Features and Interaction Terms
- 정의: predicate는 입력 $x$를 받아들여 참(True) 또는 거짓(False)으로 평가하는 함수입니다.

예를들어, $x$가 어떤 사람의 정보를 나타낸다고 하면, "$s(x) = 1$"은 그 사람이 자고 있는지 여부를 참(True) 또는 거짓(False)으로 평가하는 술어입니다.<br>
마찬가지로, "$d(x) = 1$"은 그 사람이 운전 중인지 여부를 참 또는 거짓으로 평가하는 술어입니다.<br>

이러한 술어를 가진 특징(feature)을 예로 들면, "$s(x)d(x)$"는 "그 사람이 자고 있고 운전 중인가?"라는 질문에 대한 답을 참 또는 거짓으로 평가하는 특징입니다.<br>
이러한 술어 간의 상호작용(interaction)을 표현하는 데 AND 논리 연산자를 사용합니다.<br>

```python
is_sleeping = True
is_driving = False
```

여기서 is_sleeping과 is_driving은 각각 불리언 값(True 또는 False)을 가지는 변수입니다.<br>
이 변수들은 위에서 설명한 predicate features를 표현하는 데 사용될 수 있습니다.

위의 예에서 "$s(x)d(x)$"는 두 술어가 동시에 참일 때만 참이 되므로, "그 사람이 자고 있고 운전 중인가?"라는 질문에 대한 답이 참(True)이 됩니다.<br>
이와 같이 술어와 AND 연산자를 이용하여 특징 간의 상호작용을 표현하는 것을 "predicate features and interaction terms"이라고 합니다.<br>

술어에서 or 논리 연산자는 사용할 수 있습니다. 예를 들어, 다음과 같은 술어는 "$x$가 5보다 작거나 $x$가 10보다 크면 True이고, 그렇지 않으면 False"라는 불리언 값을 반환합니다.

```python
def predicate(x):
    return x < 5 or x > 10
```
### Geometric Example: Two class problem, nonlinear boundary
[*Feature Extraction*](http://youtu.be/3liCbRZPrZA)

이 영상에서는 두 개의 클래스를 가진 이진 분류 문제를 다루고 있습니다.<br>
기존 데이터의 feature map이 (x1,x2)인 경우, 즉 2차원 평면 상에서 선형 분리가 불가능한 비선형 경계를 가진 경우를 가정합니다.<br> 
이 경우, 선형 모델을 사용하여 분류하려 하면 해결이 불가능하다는 것을 보여주고 있습니다.<br>
![image](https://user-images.githubusercontent.com/108461006/220605445-3ebec93e-cb2d-4565-a52b-0637728afba7.png)

그러나 적절한 비선형 feature map을 사용하면 ($φ(x) = x1, x2, x_1^2 + x_2^2$와 같은 형태) 더 높은 차원의 공간으로 매핑될 수 있으며, 이 경우 2차원에서는 비선형 경계가 있었지만 고차원에서는 선형 경계를 가질 수 있습니다.<br>
이렇게 선형 모델을 사용하여 고차원에서 분류를 수행할 수 있으며, 이를 통해 비선형 문제를 해결할 수 있습니다.<br>

### Expressivity of Hypothesis Space
- $φ : X → R^d: F ={f (x) = w^Tφ(x)}$<br>

가설 공간(hypothesis space)은 학습 알고리즘이 가능한 가설(모델)의 집합입니다.<br>
이 가설 공간을 얼마나 크게 만들 수 있느냐가 모델의 표현 능력(expressivity)을 결정합니다.<br>

![image](https://user-images.githubusercontent.com/108461006/220607091-bb0017bb-48c6-4d5e-8c94-b0e64e2712c2.png)

선형 가설 공간에서는 각 데이터 포인트를 특성 매핑 함수$(φ)$로 매핑한 $d$차원 벡터와 이 벡터와의 가중치$(w)$의 내적으로 표현됩니다.<br>
이 때, 가중치$(w)$는 학습 알고리즘이 학습하는 모델의 파라미터입니다.<br>

가설 공간을 확장하기 위해서는 특성 매핑 함수$(φ)$에서 더 많은 특성을 추가하면 됩니다. <br>
이렇게 확장된 가설 공간에서는 더 복잡한 가설(모델)을 학습할 수 있습니다.<br>


[*[출처] : FOUNDATIONS OF MACHINE LEARNING by Bloomberg ML EDU*](https://bloomberg.github.io/foml/#home).