---
title: Feature Extraction 3rd(Saturation)
layout: post
description: Lecture summary
use_math: true
post-image: https://github.com/sparkerhoney/sparkerhoney.github.io/blob/master/_images/machine%20learning.png?raw=true
category: machine learning
tags:
- Data Science
- machine learning
---


# Features
## Handling Nonlinearity with Linear Methods
### Saturation: The Issue
- **포화(saturation)** 가 발생하는 특징은 값이 일정 범위를 넘어설 경우, 결과에 더 이상 긍정적인 영향을 끼치지 않는 경향이 있습니다.<br>
Saturation은 입력값이 일정 수준 이상이 되면 출력값이 더 이상 증가하지 않는 현상을 말합니다.<br>
 예를 들어, 어떤 자전거 브레이크의 제동력이 브레이크 패드와 바닥 사이의 마찰 계수에 비례한다고 가정해보겠습니다.<br>
 그렇다면, 마찰 계수가 일정 수준 이상이 되면 브레이크 제동력이 더 이상 증가하지 않을 것입니다.<br>
 이러한 경우에는 선형 모델이나 로지스틱 회귀 모델 같은 단순한 모델을 사용하면 제대로된 결과를 얻을 수 없습니다.<br>
 이런 경우에는 적절한 모델링 기법이나 feature engineering 기술을 사용하여 모델을 개선해야 합니다.<br>
이 경우 모델은 특정 값 이상의 특징을 무시하거나, 잘못된 가중치를 부여할 수 있습니다.<br>

- Setting: 사용자의 검색어와 관련된 제품을 찾습니다.<br>
- Input: Product $$X$$<br>
- Action: $$X$$의 검색어와의 관련성을 점수로 평가합니다.<br>
- Feature Map: $$φ(x) = [1,N(x)]$$, where $$N(x)=$$ $$x$$를 구매한 사람들의 숫자 입니다.<br>

Feature Map에서와 같이 $$N(x)=$$인 feature를 추가해서 비선형적인 패턴을 추출하겠다는 이야기 입니다.<br> 
이 경우 우리는 $$N(x)$$와 $$x$$의 검색어와의 관련성 사이에서 단조적인 관계가 있을 것으로 예상하지만, 실제로는 아닐 수 있습니다.<br>
그 이유는 관련성 점수는 **관련성 예측에 대한 신뢰도** 를 반영하기 때문입니다.<br>

구매횟수가 매우 큰 경우에 feature에 대한 정보를 잘못해석해서 예측이 어려워지는데 이것을 우리는 **Saturation** 이라고 합니다.<br>
따라서 우리는 feature에 대한 정보를 더 잘 반영할 수 있는 방식으로 feature map을 설계하면 saturation을 피할 수 있습니다.<br>

즉, $$N(x)$$가 1000이 되었다고 해서($$x$$를 구매한 사람이 1000명 이라고 해서)$$N(x)$$가 100일 때 보다 10배 더 신뢰할 수 있다는 것이 아니라는 말입니다.<br>
그렇기 때문에 $$N(x)$$가 커질수록 관련성 점수도 증가하지만, **선형적으로 증가하는 것이 아닙니다.** (비선형적인 것을 handling해야하는 이유)<br>

데이터가 많은 것은 좋지만 그렇다고 해서 구매한 사람 수가 많을 수록 관련성 점수와의 관계가 선형적이지 않을 수 있습니다.<br>
구매한 사람의 수가 늘어날수록 관련성 예측에 대한 신뢰도는 높아지지만 그 차이가 크지 않다는 것을 의미합니다.<br>

---

### Saturation: Solve with nonlinear transform
- $$φ(x) = [1,log {1+N(x)}]$$<br>

Saturation이 발생하는 경우에는, 입력값과 출력값의 관계가 선형적이지 않아서 원하는 결과를 얻을 수 없게 됩니다.<br> 
이럴 때에는 입력값에 대해 비선형 변환을 적용하여 문제를 해결할 수 있습니다.<br>

예를 들어, 제품 검색에서 특정 제품과 검색어의 관련성을 예측할 때, 제품이 구매된 횟수를 입력값으로 사용할 수 있습니다.<br>
하지만 구매 횟수가 큰 차이를 보이는 경우(1000번과 100번의 예시), 입력값과 관련성 점수 간에 선형적인 관계를 가정하는 것이 어렵습니다.<br>
따라서 이 때에는 로그 함수와 같은 비선형 함수를 사용하여 입력값을 변환하면 선형적인 관계를 더 잘 모델링할 수 있습니다.<br>
예를 들어, $$[1, log(1+N(x))]$$와 같은 비선형 특징 맵을 사용할 수 있습니다.<br>
이때, 로그 함수의 밑(base)은 어떤 값이던지 상관없이 원하는 결과를 얻을 수 있습니다.<br>

---

### Saturation: Solve by discretization

Discretization은 연속적인 값을 구간별로 나누어 각 구간에 해당하는 이산적인 값으로 변환하는 과정을 말합니다.<br>
이산적인 값을 다루는 것이 훨씬 더 쉽고 간편하기 때문에(연속적인 값을 다루려면 무한한 경우의 수를 고려해야함), 이산적인 값을 다루는 것이 더 효율적일 때 사용됩니다.<br>

- Discretization (a discontinuous transformation):<br>
  $$φ(x) = (1(5 \le N(x) < 10),1(10 \le N(x) < 100),1(100 \le N(x)))$$<br>

- Sometimes we might prefer one-sided buckets:<br>
  $$φ(x) = (1(5 \le N(x)),1(10 \le N(x)),1(100 \le N(x)))$$<br>

이 방법은 구매 횟수(N(x))에 따라 몇 개의 분리된 버킷(buckets)으로 나누는 것입니다.<br> 여기서, 각 버킷은 구매 횟수가 특정 구간에 속하는 제품들을 나타냅니다.<br>

예를 들어, 첫 번째 예시에서는 5회 이상 10회 미만으로 구매된 제품은 1번째 버킷, 10회 이상 100회 미만으로 구매된 제품은 2번째 버킷, 100회 이상으로 구매된 제품은 3번째 버킷에 속합니다.<br>
이 방법은 몇몇 버킷들을 제외하고는 비선형적인 관계를 표현할 수 있습니다.<br>

하지만, 모든 버킷의 크기가 비슷하도록 하기 위해 한쪽 방향으로만 버킷을 나누는 경우가 있습니다.<br>
예를 들어, 두 번째 예시에서는 각 버킷의 크기가 달라지도록 5회 이하로 구매된 제품은 1번째 버킷, 10회 이하로 구매된 제품은 2번째 버킷, 100회 이하로 구매된 제품은 3번째 버킷에 속하게 됩니다.<br>

이 방법은 구매 횟수가 적은 제품에 대한 모델의 가중치를 규제할 수 있어 일반화 성능을 높일 수 있습니다.<br>

> **가중치** 는 어떻게 **규제** 하는데?<br>
  위의 모델에서 사용하는 구매횟수(n(x)) 변수는 값이 연속적이지 않고, 몇 개의 구간으로 나누어져 이산화(discretization)되어 있습니다.<br>
  예를 들어, 5 이상 10 미만인 구매 횟수를 가진 제품은 첫 번째 버킷(구간)에 속하며, 10 이상 100 미만인 구매 횟수를 가진 제품은 두 번째 버킷에 속합니다.<br>
  그러나 이러한 구간은 실제 데이터의 분포에 따라 큰 차이를 보일 수 있습니다.<br>
  예를 들어, 구매횟수가 100 이상인 제품은 적은 비중을 차지할 수 있습니다.<br>
  따라서 이러한 경우, 두 번째 모델과 같이 한쪽 방향으로만 버킷을 만드는 것이 더 적합할 수 있습니다.<br>
  또한, Saturation은 모델의 가중치를 규제하는 방법 중 하나입니다.<br>
  이 방법은 **구매횟수가 적은 제품** 에 대해 모델의 가중치를 규제하여 *일반화 성능* 을 높이는 것입니다.<br>
  예를 들어, 구매횟수가 5인 제품과 같은 희귀한 경우에는 구매횟수가 더 높은 제품에 비해 더 큰 가중치를 주는 것을 방지할 수 있습니다.<br>
  즉, *구매횟수 분포에 따라서 적합한 버킷 방식* 을 선택하고, Saturation을 적용하는 것은 모델의 일반화 성능을 높이는 방법 중 하나입니다.<br>
  이를 통해 구매횟수 분포가 크게 차이 나는 데이터에 대해 더 정확한 예측을 할 수 있습니다.<br>

하지만, 버킷의 수가 많아지면 *모델의 복잡도* 가 증가하므로 과적합 문제를 방지하기 위해 조절이 필요합니다.<br>

위에서 주어진 feature map인 $$φ(x) = [1, N(x)]$$에서, 구매 횟수인 $$N(x)$$를 그대로 사용하면 모델이 구매 횟수에 대한 선형적인 관계만 학습할 수 있습니다.<br>

하지만, 구매 횟수를 여러 개의 구간으로 나누어 이산적인 값으로 만들면, 각 구간에 따라 다른 가중치를 학습하여 **더 복잡하고 유연한 모델** 을 만들 수 있습니다.<br>

위의 예시에서는 3개의 구간으로 나누어 $$φ(x)$$를 정의했습니다.<br>
이러한 변환 방법을 **"discretization"** 이라고 부르며, 이산적인 값으로 변환하여 모델을 더 유연하게 만들 수 있습니다.<br>

> 모델을 유연하게 만들면 *장점* 이 뭔데?<br>
  모델을 유연하게 만들어주는 것은 여러 가지 장점을 가질 수 있습니다.<br>
  첫째, 데이터의 복잡도에 더욱 잘 적응할 수 있습니다.<br>
  모델이 더 유연해지면 데이터의 미세한 패턴을 더 잘 파악하고 예측할 수 있습니다.<br> 둘째, 모델이 일반화 성능을 더 잘 발휘할 수 있습니다.<br>
  데이터에 과적합되는 것을 방지하여 새로운 데이터에 대한 예측 성능을 높일 수 있습니다.<br> 
  셋째, 다양한 feature를 적용하여 모델의 표현력을 높일 수 있습니다.<br>
  더 다양한 feature를 사용하면 모델이 다양한 패턴을 학습할 수 있어 예측 성능이 높아질 수 있습니다.<br>

---

[*출처 : FOUNDATIONS OF MACHINE LEARNING by Bloomberg ML EDU*](https://bloomberg.github.io/foml/#home).<br>

---